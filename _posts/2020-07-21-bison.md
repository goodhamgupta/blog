---
toc: true
layout: post
description: A new framework for information retreival from documents
author: Shubham Gupta
comments: true
categories: [nlp, transformer, review]
title: BERT + BM25 = BISON
---

# Introduction
- The aim of this paper is to create a framework to map query and doc into semantic vectors via self-attention models.
- We cant use prior knowledge about important tokens for models based on self attention.
  - Words are split into different tokens using a tokenization mechanism such as WordPiece. We cannot translate word-level knowledge into different tokens.
- However, from classical information retrieval, we know that prior knowledge about the word is important. For example, ERNIE used a Knowledge Graph to acheive SOTA on several GLUE tasks.
- Furthermore, documents have different fields with varying degrees of importance such as text, header, filetypes, etc. We cannot combine these fields directly because their importance varies with respect to a task. 
- **Key takeaways**:
    - Combine BM25 to learn attention scores with Query(Q) and Key(K) matrices, which are used in self-attention.
    - Word weight sharing to reduce knowledge discrepancy between tokens and words.
    - Combine multiple fields by placing different fields in different segments using a BM25F, a variation of BM25.

# Background
- Using NN for doc retrieval has 2 approaches
  - **Siamese Networks**: In this, we encode the given query $q$ and the document $d$ seperately.
  - **Interactive Networks**: In this, we encode the given query $q$ and the document $d$ together.
- For large scale document retrieval tasks dependent on vector search, siamese networks are preferred since we can encode multiple documents without a query offline. This ensures the overall document retrieval process is fast in production.
- BISON is built using a Siamese Network architecture.

# Proposed method
## Overview of BISON
- four parts
  - **Word level BM25**: prepend CLS to query and use combined fields rep
  - **Token level representation**: Use token, postion and segment embedding
  - **BISON Encoder**: Encodes q and d into semantic spacy by siamese structure making online seving popssbile. 3 stacked layers of BISON layers.
  - **Matching score**: cosine similarity

## BISON Encoder: Weighted Self Attention
- As we know from the original "Attention" paper, attention is computed using the query, key and value matrices. 
- To the above, we will add the importance of tokens via BM25. We will introduce w_i and multiply with above attention to get new attention score i.e Weighted Self Attention
    $$ A_{ij}^w = w_j\frac{q_i.k_j^T}{\sqrt{d}} $$

![Weighted Self attention]({{site.baseurl}}/images/bison/weighted_self_attention.png)
- Mathematically, it is represented as:

$$WeightedSelfAttention(Q,K,W,V) = softmax(W (.) \frac{QK^T}{\sqrt{d}}V$$

- WSA is the main block unit. Multiple such units are tacked to get the multi-head structure.
- Rescaling by $W^o$, we get **Complex Weighted Self Attention(CWSA)**.
- Fully connected layer is added. In both CWSA and fully connected layyer, layer norm and residual connections are used

$$CWSA = Concat(WeightedSelfAttention1,... WeightedSelfAttention, n)W^o$$

$$CWSA_{out}=LayerNorm(CWSA + X)$$

$$BISONEncoder = LayerNorm(CWSA_{out} + FeedForward(CWSA_{out}))$$

## BM25 Weight generation
- Use BM25 for weight scores in query and BM25F for weight scores in multi field documents
- BM25F, a variation of BM25, is for documents with different fields, each having different importance in term of relevance saturation and length normalization. For additional details, see the file [here](https://web.stanford.edu/class/cs276/handouts/lecture12-bm25etc.pdf).

### Inherent Query BM25
- For a given query, BM25 is calculated within query.

$$
w_i^{BM25} = idf_i \frac{tf_i}{tf_i + k_1(1-b+b \frac{l_q}{avl_q})}
$$

- $l_q$ - query length and $avl_q$ - query average length along collection

### Inherent Document BM25F
- BM25F is implemented by assigning different degrees of importance to the different zones in a document such as title, header, footer, filetype, text, etc. For a  $word_j$ in a document field $c$, it's frequency $f_j^c$ is defined as:

$$
atf_j^c = \frac{fw_c . tf_j^c}{1.0 + fln_c . (\frac{fl_c}{avl_c}-1.0)}
$$

- The corresponding BM25F score is computed as

$$
w_j^{BM25F} = idf_j\frac{atf_j}{k_1 + atf_j}
$$
## Whole word weight sharing

- BERT uses wordpiece to produce tokens from raw text. However, because of this, we cannot directly apply the prior knowledge we obtained from BM25.
- **Solution**: Assign the _same_ word weight to all tokens for a given word. This way, a token might have different weight depending on the context of the given word.

## Combined Fields Representation
