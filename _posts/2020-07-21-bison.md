---
toc: true
layout: post
description: A new framework for information retreival from documents
author: Shubham Gupta
comments: true
categories: [nlp, transformer, review]
title: BISON = BERT + BM25 
---

# Introduction
- Map query and doc into semantic vectors via self-attention models
- cant use prior knowledge about important tokens => imp for info retrieval tasks
    - rare word split into different tokens. can't translate word-level knowledge into different tokens
- different fields => can't combine directly because hetrogenous
- key takeaways:
    - combine bm25 to learn attention scores with Q and K
    - word weight sharing to reduce knowledge discrepancy between tokens and words
    - combine multiple fields by placing different fields in diff regions(what?)

# Paper intro
- In BERT, token score is influeced by all tokens. BUT, in IR, we know some words have more weight. Example: ERNIE used Knowledge graph and acheived sota
- precompute BM25 scores for query and document, use weight to perform self attention
- use 30k token vocb from wordpeice. Weight sharing between tokens and words
- combine multiple fields to encode document in unified vector space thereby reducing storage and computation. 

# Background
- Using NN for doc retrieval has 2 approaches
  - siamese networds => encode q and d seperately
  - Interactie networks => encode q and d together
- For large scale doc retrieval tasks dependent on vector search, siamese preferred since we can encode multiple docs without a query offline
- BISON build on siamese network

# Proposed methods
## Overview of BISON
- four parts
  - word level BM25: prepend CLS to query and use combined fields rep
  - Token level rep: Use token, postion and segment embedding
  - BISON Encoder: Encodes q and d into semantic spacy by siamese structure making online seving popssbile. 3 stacked layers of BISON layers.
  - Matching score: cosine similarity

## BISON Encoder: Weighted Self attention
- As we know from the original "Attention" paper, attention is computed using the query, key and value matrices. 
- To the above, we will add the importance of tokens via BM25. We will introduce w_i and multiply with above attention to get new attention score i.e Weighted Self Attention
    $$ A_{ij}^w = w_j\frac{q_i.k_j^T}{\sqrt{d}} $$
- ![Weighted Self attention]({{site.baseurl}}/images/bison/weighted_self_attention.png)
$$WeightedSelfAttention(Q,K,W,V) = softmax(W (.) \frac{QK^T}{\sqrt{d}}V$$
- WSA is the main block unit. Multiple such units are tacked to get multi-head structure
- Rescaling by $W^o$, we get complex weighted self attention(CWSA)
- Fully connected layer is added. In both CWSA and fully connected layyer, layer norm and residual connections are used

$$ CWSA = Concat(WEightedSelfAttention1,... WeightedSelfAttention, n)W^o$$
$$CWSA_{out}=LayerNorm(CWSA + X)$$
$$BISONEncoder = LayerNorm(CWSA_{out} + FeedForward(CWSA_{out}))$$

## BM25 Weight generation
- Use BM25 for weight scores in query and BM25F for weight scores in multi field documents
- BM25F is for docs with different fields, each having different importance
- BM25 is based on tf and idf
- For query, BM25 is calculated within query.

$$
w_i^{BM25} = idf_i \frac{tf_i}{tf_i + k_1(1-b+b \frac{l_q}{avl_q})}
$$
- $l_q$ - query length and $avl_q$ - query average length along collection
