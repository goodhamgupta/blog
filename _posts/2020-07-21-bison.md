---
toc: true
layout: post
description: A new framework for information retreival from documents
author: Shubham Gupta
comments: true
categories: [nlp, transformer, review]
title: BISON = BERT + BM25 
---

# Introduction
- Map query and doc into semantic vectors via self-attention models
- cant use prior knowledge about important tokens => imp for info retrieval tasks
    - rare word split into different tokens. can't translate word-level knowledge into different tokens
- different fields => can't combine directly because hetrogenous
- key takeaways:
    - combine bm25 to learn attention scores with Q and K
    - word weight sharing to reduce knowledge discrepancy between tokens and words
    - combine multiple fields by placing different fields in diff regions(what?)

# Paper intro
- In BERT, token score is influeced by all tokens. BUT, in IR, we know some words have more weight. Example: ERNIE used Knowledge graph and acheived sota
- precompute BM25 scores for query and document, use weight to perform self attention
- use 30k token vocb from wordpeice. Weight sharing between tokens and words
- 
