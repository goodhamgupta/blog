{
  
    
        "post0": {
            "title": "How much do you know?",
            "content": "Introduction . This is a new paper which explores the limits of using their new T5 titled How Much Knowledge Can You Pack Into The Parameters of a Language Model?. model in a context-free QA domain. . | As with the T5 model itself, it is very interesting to see these one-model-to-rule-them-all architectures as they exhibit some form of generalization. . | I found this paper from Adam Roberts twitter thread which is available here . | Core Idea: This paper will test two main things: . How well does the model create a knowledge base such that it can answer questions just based on this base and no other information. . | Do model with more parameters store more information? Measuring knowledge retreiving ability is used to check this point. . | . | . Paper Introduction . Reading Comprehension: Given a question and context, lookup and give the answer. . | Open domain question answering: Random context-independent questions. It is given entire context(all the information possible in the world) and the model is expected to deduce the answer. Open book exam. . | Here, problem is similar to open book exam + no context given at all. Model should retreive info from parameters and return the values. Closed book exam. . | T5: Treat every NLP task as text-to-text problem using encoder decoder Transformer. . | For natural questions dataset, evaluation is done as follows: . | First method: . Ignore all “unanswerable” and “long answer” type questions. . | model trained to output single answer . | Questions with answers longer than 5 tokens are ignored . | Answers normalized before comparsion . | Answer is correct if it matches any of the annotated answers . | . | Second method: . Considered correct only if model predicts all the answers correctly | . | For fine tuning, use AdaFactor Optimizer(need to read more about this one) . | . Results . SOTA on Natural Questions(NQ) and WebQuestions(WQ) dataset. Worst performance on TriviaQA(TQA). . | Performance increases with model size. . | Guu et all(2020) performs better than T5 on NQ and WQ. Need to read this paper as well. It . Retreives Revevant documents . | Answers questions in end-to-end fashion . | . | Closed-book model seem to perform on par with open-book models, leading to new research directions. . | For multiple answer type questions, T5 lower than SOTA BUT much better than baseline that was published with the paper. Therefore, T5 can perform well on these types of questions as well. . | . Drawbacks . Model is far too expensive to train. . | Open-book models provide some indication of what information was used to answer the problem. HOWEVER, T5 just has a distribution over parameters that cannot be interpreted. . | MLE does not gurantee the model will learn a fact. Therefore, difficult to ensure the model learns specific information during pre-training . | Measure and improve performance on difficult QA tasks like DROP, which needs reasoning ability. . | .",
            "url": "https://goodhamgupta.github.io/blog/nlp/language_model/review/2020/04/21/knowledge-lm.html",
            "relUrl": "/nlp/language_model/review/2020/04/21/knowledge-lm.html",
            "date": " • Apr 21, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Attention is all you need",
            "content": "Introduction . This paper review is following the blog from Jay Alammar’s blog on the Illustrated Transformer. The blog can be found here. | . Paper Introduction . New architecture based solely on attention mechanisms called Transformer. Gets rids of recurrent and convolution networks completely. . | Generally, RNN used to seq-to-seq tasks such as translation, language modelling, etc. . | Transformer allows for significant parallelization and relies only on attention. . | . Background . Self attention Attention to different positions of a sequence in order to compute a representation of the sequence. | . Model Architecture . Transformer uses the following: . Encoder decode mechanism . | Stacked self attention . | Point wise fully connected layer for encoder and decoder . | . . | . Encoder and decoder stacks . Encoder: 6 identical layers. 2 sub layers per layer . | First: multi-head self attention mechanism . | Second: Fully connected feed forward network . | Apply residual connection for each of the two laters . | Apply layer normalization . | Decoder: 6 identical layers. 2 sub layers as above + 1 more which performs multi-head attention over output of encoder stack . | Residual blocks: Present around all 3 sub layers . | Layer normalization: Normalizes input across features instead of normalizing input features across batch dimension(i.e in batch normalization). There is a great overview of normalization layers available by Akash Bindal here. . | Modify self-attention sub layer to prevent positions from attending to subsequent positions. Ensures that i output depends only on words before i. . | . Attention . 3 vectors: Query(Q), Key(K) and Value(V) . | Output = Weighted sum of values. Weights assigned as a function of query with key. . | Scaled dot-product attention and multi-head attention . . | Attention is calculated as: . Attention(Q,K,V)=softmax(QKTdk)VAttention(Q,K,V) = softmax( frac{QK^T}{ sqrt{d_k}})VAttention(Q,K,V)=softmax(dk​​QKT​)V | Dot product attention is faster and more space-efficient than additive attention. . | . Multi head attention . Using multile q, k and v vectors. Get the final output, concatenate them and get another final projection $d_{v}$. . MultiHead(Q,K,V)=Concat(head1,...,headh)WOwhere headi=Attention(QWiQ,KWiK,VWiV)MultiHead(Q,K,V) = Concat(head_1,...,head_h)W^O text{where } head_i = Attention(QW_{i}^{Q}, KW_{i}^{K},VW_{i}^{V})MultiHead(Q,K,V)=Concat(head1​,...,headh​)WOwhere headi​=Attention(QWiQ​,KWiK​,VWiV​) | Dimensions of the key and value matrices will be: $d_{k} = d_{v} = d_{model}/h = 64$ . | . Applications of attention . Encoder-decoder attention: Q from previours decoder, K and V from output of decoder. Attend to all positions in the input sequence. . | Encoder: Self attentnion laters. Q,K and V from output of previous layer in the encoder. Some talk about leftward flow, didn’t really understand this bit. Will come back to this in sometime. . | . Position-wise Feed-Forward Networks . Each layer contains feed-forward network. . FFN(x)=max(o,xW1,+b1)W2+b2FFN(x) = max(o, xW_1,+ b_1)W_2 + b_2FFN(x)=max(o,xW1​,+b1​)W2​+b2​ | . Embeddings and Softmax . Convert input and output string to vectors of dim $d_{model}$ . | Share weight matrix between two embedding layers and the pre-softmaax linear transformation . | . Positional Encoding . Encode positions of the tokens for the input and output. . | Same vector size i.e $d_{model}$ . PE(pos,2i)=sin(pos/100002i/dmodel)PE(pos,2i+1)=cos(pos/100002i/dmodel)PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}}) PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})PE(pos,2i)​=sin(pos/100002i/dmodel​)PE(pos,2i+1)​=cos(pos/100002i/dmodel​) | Might allow approximation of longer sequence lenghts than seen in the training set . | . Why self attention? . Total computational complexity per layer . | Parallel Computation . | Path length between long-range dependencies in the network. . | . Training . Optimizer . Use Adam. Vary learning rate according to formula: $lrate = d_{model}^{-0.5} . min(step_num^{-0.5}, step_num . warmupsteps^{-1.5})$ . | Increase LR for warmup steps, then decrease propotionally to inverse square root of step number. Warmup steps = 4000 . | . Regularization . Residual Dropout . | Label Smoothing: Instead of using 0 and 1 as class labels, allow for some uncertainity in the prediction, and use values like 0.1 and 0.9 for the classes . | . Conclusion . This was the first model based entirely on attention. It acheived SOTA results on Machine Translation and English contituency parsing. . | Admittedly, there are still a lot of bits I don’t really understand. Specially around self attention. I will give this paper another read after going through Jay Alammar’s blog. . | .",
            "url": "https://goodhamgupta.github.io/blog/nlp/attention/review/2020/04/20/attention.html",
            "relUrl": "/nlp/attention/review/2020/04/20/attention.html",
            "date": " • Apr 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "REALM: Retrieval-Augmented Language MOdel Pre-Training",
            "content": "Introduction . REALM is a paper mentioned in the T5 paper titled: How Much Knowledge Can You Pack Into The Parameters of a Language Model? . | TLDR: This paper retrieves documents that have the information present while solving Question-Answer type problems. . NOTE: This post is more like my running notes while reading the paper than a comprehensive blog. I will update this blog once I learn a little more about the transformer architecture. . | Introduced a latent knowledge retriever, which can attend and retrieve documents over large corpus and can be trained in unsupervised manner using masked language modelling technique and backprop through retreiver which considers lots of docs. . . | Key point: Train retriever using a performance-based signal from unsupervised text. . | Retrieval based LM =&gt; Moar computational resources =&gt; Moar money . Solution: Computation performed for each doc is cached and can be used again. Best doc selected using Maximum Inner Product Search(MIPS). Read the paper here. | . | REALM retriever can be used on downstream tasks via transfer learning. . | REALM is SOTA on NQ-Open, WQ and CuratedTrec. . | . Approach . Retreive-then-predict generative process . Training: Masked-LM. Fine-tuning: Open QA task . | Computing chance of the document given a question decomposed into two steps: . Function to be computed: p(y∥x)p(y |x)p(y∥x) . | Given xxx,retrive documents zzz from corpus ZZZ. Modelled as: p(z∥x)p(z |x)p(z∥x) . | Condition of both zzz and xxx to generate output yyy i.e p(y∥z,x)p(y |z, x)p(y∥z,x) . | Overall likelihood yyy is generated by treating zzz as latent variable and marginalizing over all documents zzz . p(y∥x)=∑zϵZp(y∥z,x)∗p(z∥x)p(y |x) = sum_{z epsilon Z} p(y |z, x) * p(z |x)p(y∥x)=zϵZ∑​p(y∥z,x)∗p(z∥x) | . | . Architecture . Neural Knowledge Retriever which models the distribution: $p(z|x)$ . | Knowledge Augmented Encoder which models the distribution p(y∥z,x)p(y |z, x)p(y∥z,x) . | . Neural Knowledge Retriever . Dense inner product model. . p(z∥x)=exp(f(x,z))∑z′exp(f(x,z′))f(x,z)=Embedinput(x)TEmbeddoc(z) begin{aligned} p(z |x) = frac{exp(f(x,z))}{ sum_{z&amp;#x27;}{exp(f(x,z&amp;#x27;))}} f(x,z) = Embed_{input}(x)^TEmbed_{doc}(z) end{aligned}p(z∥x)=∑z′​exp(f(x,z′))exp(f(x,z))​f(x,z)=Embedinput​(x)TEmbeddoc​(z)​ | EmbedinputEmbed_{input}Embedinput​ and EmbeddocEmbed_{doc}Embeddoc​ are embedding functions . | f(x,z)f(x,z)f(x,z) is called relevance score. It is inner product of vector embeddings. . | Relevant Distribution is softmax over all relevance scores . | Embedding implement using BERT-style transformers. Join using &lt;SEP&gt;, prefix using &lt;CLS&gt; and append &lt;SEP&gt; as the end token. joinBERT(x)=[CLS]x[SEP]joinBERT(x1,x2)=[CLS]x1[SEP]x2[SEP] begin{aligned} join_{BERT}(x) = [CLS]x[SEP] join_{BERT}(x_1, x_2) = [CLS]x_1[SEP]x_2[SEP] end{aligned}joinBERT​(x)=[CLS]x[SEP]joinBERT​(x1​,x2​)=[CLS]x1​[SEP]x2​[SEP]​ . | Pass above into transformer, which gives over vector for each token. Perform linear projection to reduce dimensionality of vector Embedinput(x)=WinputBERTCLS(joinBERT(x))Embeddoc(z)=WdocBERTCLS(joinBERT(ztitle,zbody)) begin{aligned} Embed_{input}(x) = W_{input}BERT_{CLS}(join_{BERT}(x)) Embed_{doc}(z) = W_{doc}BERT_{CLS}(join_{BERT}(z_{title}, z_{body})) end{aligned}Embedinput​(x)=Winput​BERTCLS​(joinBERT​(x))Embeddoc​(z)=Wdoc​BERTCLS​(joinBERT​(ztitle​,zbody​))​ . | . Knowledge-Augmented Encoder . Given input xxx and relevant doc zzz, this defines p(y∥z,x)p(y |z,x)p(y∥z,x) . | Join xxx and zzz into single sequence and feed into transformer . | Here, training is different for pre-training vs fine-tuning . For pre-training, predict [MASK] token. Use same Masked LM(MLM) loss as in Transformer(Devlin et al.) . | For Open-QA, we need to produce string yyy. | Assumption: yyy occurs as sequence of tokens in some document in the corpus. | . | . Training . Compute gradients in θ thetaθ and ϕ phiϕ and optimize using SGD. . | Challenge: Computing p(y∥x)p(y |x)p(y∥x) . | Approx by summing over top kkk documents with highest prob under p(z∥x)p(z |x)p(z∥x) . | Question: How to find top kkk docs? Answer: Use MIPS . | Need to precompute Embeddoc(x)Embed_{doc}(x)Embeddoc​(x) for all docs. Problem? It changes with each step of SGD. . | Solution: Async refresh $Embed_{doc}$ every 500 steps . | Use MIPS to select top $k$ docs. For these docs, recompute $p(z|x)$ using new $ theta$. . | . Implementing async MIPS refreshes . Two jobs running in parallel: . Primary trainer: Perform gradient updates on parameters . | Secondary index builder: Embeds and indexes the docs . . | Async refresh used only for pre-training . | For fine tuning, build index once from pre-trained $ theta$ and use it. . | . | . What does retriever learn? . Retriever promotes docs that improve accuracy . | This can be analyzed by analyzing gradient wrt the parameters . | . Injecting inductive biases into pre-trianing . Salient span masking: Some questions require only local context. Select named entities and dates and mask one of them. Performs better. . | Null document: Add null document to top kkk documents to allow answers even when no context is required . | Prohibiting trivial retrievals: If knowledge corpus ZZZ is the same as pre-training corpus XXX, it can predict yyy by looking at xxx in zzz. Exclude trivial candidate . | Initialization: Warm up EmbedinputEmbed_{input}Embedinput​ and EmbeddocEmbed_{doc}Embeddoc​ using Inverse Cloze Task(ICT) i.e model trained to retrieve the doc where the sentence came from. . | . Experiments . REALM outperforms all approaches by a big margin. | . Future Work . Structured knowledge where we learn entities which are informative . | Multi lingual setting. Retreiving knowledge in high resource language to better represent text in low resource language . | Multi model setting. Retrieve images or videos that can provide knowledge not present in text . | . Comments . Overall, I enjoyed reading this paper. However, there are two key points that concern me: . The authors mention using MIPS for selecting the top kkk documents, in order to simplify the task. However, would selecting only these documents from the entire dataset not lead to some information loss? I would like to see more experiments around this area. | There are no experiments around trying out larger models. While I agree that T5 is the largest model available right now, there is no evidence given that a model larger than T5-large would not perform better than the current REALM model. I would like to see some more exploration around this area. | . Resources . There are a number of other resources you can use to learn more about this paper such as: . The original paper available here | Tweet summary by Adam Roberts available here | Video summary by Václav Košař available here | Huggingface Reading group summary by Joe Davidson available here | .",
            "url": "https://goodhamgupta.github.io/blog/nlp/bert/review/2020/03/14/realm.html",
            "relUrl": "/nlp/bert/review/2020/03/14/realm.html",
            "date": " • Mar 14, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Bayesian Golf Putting Model",
            "content": "Introduction . Disclaimer . This is inspired from Dr. Andrew Gelman&#39;s case study, which can be found here. Specifically: . This is heavily inspired by Colin Caroll&#39;s Blog present here. A lot of the plotting code from his blog post has been reused. | Josh Duncan&#39;s blog post on the same topic which can be found here. | . This is not a novel solution. It is merely a replication of Dr. Gelman&#39;s blog in PyMC3. . Problem . This is based on a popular blog post by Dr. Andrew Gelman. Here, we are given data from professional golfers on the proportion of success putts from a number of tries. Our aim is to identify: . Can we model the probability of success in golf putting as a function of distance from the hole? . EDA . import pandas as pd import numpy as np import pymc3 as pm import matplotlib.pyplot as plt import seaborn as sns . WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions. . The source repository is present here . data = np.array([[2,1443,1346], [3,694,577], [4,455,337], [5,353,208], [6,272,149], [7,256,136], [8,240,111], [9,217,69], [10,200,67], [11,237,75], [12,202,52], [13,192,46], [14,174,54], [15,167,28], [16,201,27], [17,195,31], [18,191,33], [19,147,20], [20,152,24]]) df = pd.DataFrame(data, columns=[ &#39;distance&#39;, &#39;tries&#39;, &#39;success_count&#39; ]) . df . distance tries success_count . 0 2 | 1443 | 1346 | . 1 3 | 694 | 577 | . 2 4 | 455 | 337 | . 3 5 | 353 | 208 | . 4 6 | 272 | 149 | . 5 7 | 256 | 136 | . 6 8 | 240 | 111 | . 7 9 | 217 | 69 | . 8 10 | 200 | 67 | . 9 11 | 237 | 75 | . 10 12 | 202 | 52 | . 11 13 | 192 | 46 | . 12 14 | 174 | 54 | . 13 15 | 167 | 28 | . 14 16 | 201 | 27 | . 15 17 | 195 | 31 | . 16 18 | 191 | 33 | . 17 19 | 147 | 20 | . 18 20 | 152 | 24 | . The variables have the following format: . Variable Units Description . distance | feet | Distance from the hole for the putt attempt | . tries | count | Number of attempts at the chosen distance | . success_count | count | The total successful putts | . Lets try to visualize the dataset: . df[&#39;success_prob&#39;] = df.success_count / df.tries . sns.set() plt.figure(figsize=(16, 6)) ax = sns.scatterplot(x=&#39;distance&#39;, y=&#39;success_prob&#39;, data=df, s=200) ax.set(xlabel=&#39;Distance from hole(ft)&#39;, ylabel=&#39;Probability of Success&#39;) . [Text(0, 0.5, &#39;Probability of Success&#39;), Text(0.5, 0, &#39;Distance from hole(ft)&#39;)] . We can notice that the probability of success decreases as the distance increases. . Baseline Model . Let us try to see we can fit a simple linear model to the data i.e Logsitic Regression. We will be using PyMC3. . Here, we will attempt to model the success of golf putting by using the distance as an independant(i.e predictor) variable. The model will have the following form: . $$y_i sim binomial(n_j, logit^{-1}(b_0 + b_1x_j)), text{for } j = 1,...J $$ . with pm.Model() as model: b_0 = pm.Normal(&#39;b_0&#39;, mu=0, sd=1) b_1 = pm.Normal(&#39;b_1&#39;, mu=0, sd=1) y = pm.Binomial( &#39;y&#39;, n=df.tries, p=pm.math.invlogit(b_0 + b_1 * df.distance), observed=df.success_count ) . Why are we using inverse logit? . Logit is a function used to convert a continous variable to a value in the range [0,1] | Inverse Logit: Used to convert real valued variable to a value in the range [0,1] | . pm.model_to_graphviz(model) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 cluster19 19 b_0 b_0 ~ Normal y y ~ Binomial b_0&#45;&gt;y b_1 b_1 ~ Normal b_1&#45;&gt;y with model: trace = pm.sample(1000, tune=1000, chains=4) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 2 jobs) NUTS: [b_1, b_0] Sampling 4 chains, 0 divergences: 100%|██████████| 8000/8000 [00:06&lt;00:00, 1164.22draws/s] The acceptance probability does not match the target. It is 0.889539212527967, but should be close to 0.8. Try to increase the number of tuning steps. The acceptance probability does not match the target. It is 0.6968711559119489, but should be close to 0.8. Try to increase the number of tuning steps. The number of effective samples is smaller than 25% for some parameters. . pm.summary(trace)[[&#39;mean&#39;, &#39;sd&#39;, &#39;mcse_mean&#39;, &#39;mcse_sd&#39;, &#39;ess_mean&#39;, &#39;r_hat&#39;]] . mean sd mcse_mean mcse_sd ess_mean r_hat . b_0 2.226 | 0.060 | 0.002 | 0.001 | 926.0 | 1.01 | . b_1 -0.255 | 0.007 | 0.000 | 0.000 | 838.0 | 1.01 | . pm.traceplot(trace) . array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fdfef7cfd68&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fdfef77efd0&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fdfef848828&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fdfef758a58&gt;]], dtype=object) . pm.plot_posterior(trace) . array([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fdfedd70be0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fdfedd2a860&gt;], dtype=object) . From the above results, we can see: . PyMC3 has estimated $b_0$ to be $2.23 pm 0.057$ | $b_1$ to be $-0.26 pm 0.007$ | . | The MCSE is almost 0 $ implies$ The simulation has run long enough for the chains to converge. | $r _hat = 1.0$ tells us that the chains have mixed well i.e hairy hedgehog pattern. | . Let us plot the final output of this model and check it with our training data. . with model: posterior_trace = pm.sample_posterior_predictive(trace) . 100%|██████████| 4000/4000 [00:04&lt;00:00, 888.12it/s] . posterior_success = posterior_trace[&#39;y&#39;] / df.tries.values . df[&#39;posterior_success_prob&#39;] = pd.DataFrame(posterior_success).median() df[&#39;posterior_success_prob_std&#39;] = pd.DataFrame(posterior_success).std() . sns.set() plt.figure(figsize=(16, 6)) prob = df.success_count/df.tries ax = sns.scatterplot(x=&#39;distance&#39;, y=df.success_prob, data=df, s=200, label=&#39;actual&#39;) # ls = np.linspace(0, df.distance.max(), 200) # for index in np.random.randint(0, len(trace), 50): # ax.plot( # ls, # scipy.special.expit( # trace[&#39;b_0&#39;][index] * ls + trace[&#39;b_1&#39;][index] * ls # ) # ) sns.scatterplot(x=&#39;distance&#39;, y=df.posterior_success_prob, data=df, label=&#39;predicted&#39;,ax=ax, color=&#39;red&#39;, s=200) sns.lineplot(x=&#39;distance&#39;, y=df.posterior_success_prob, data=df,ax=ax, color=&#39;red&#39;) ax.set(xlabel=&#39;Distance from hole(ft)&#39;, ylabel=&#39;Probability of Success&#39;) . [Text(0, 0.5, &#39;Probability of Success&#39;), Text(0.5, 0, &#39;Distance from hole(ft)&#39;)] . The curve fit is okay, but it can be improved. We can use this as a baseline model. In reality, each of them is not a point, but an posterior estimate. Because the uncertainity is small(as seen above), we&#39;ve decided to show only the median points. . From the above model, putts from 50ft are expected to be made with probability: . import scipy res = 100 * scipy.special.expit(2.223 + -0.255 * 50).mean() print(np.round(res, 5),&quot;%&quot;) . 0.00268 % . Modelling from first principles . Geometry based Model . . We&#39;ll try to accomodate the physics associated with the problem. Specically, we assume: . Assumptions . The golfers can hit the ball in any direction with some small error. This error could be because of inaccuracy, errors in the human, etc. | This error refers to the angle of the shot. | We assume the angle is normally distributed. | . Implications . The ball goes in whenever the angle is small enough for it to hit the cup of the hole! | Longer putt $ implies$ Larger error $ implies$ Lower success rate than shorter putt | . From Dr. Gelman&#39;s blog, we obtain the formula as: . $Pr(|angle| &lt; sin^{-1}( frac{(R-r)}{x})) = 2 phi big( frac{sin^{-1} frac{R-r}{x}}{ sigma} big) - 1$ . $ phi implies$ Cumulative Normal Distribution Function. . Hence, our model will now have two big parts: . $$y_j sim binomial(n_j, p_j)$$ . $$p_j = 2 phi big( frac{sin^{-1} frac{R-r}{x}}{ sigma} big) - 1$$ . Typically, the diameter of a golf ball is 1.68 inches and the cup is 4.25 inches i.e . $$r = 1.68 text{inch}$$ $$R = 4.25 text{inch}$$ . ball_radius = (1.68/2)/12 cup_radius = (4.25/2)/12 . def calculate_prob(angle, distance): &quot;&quot;&quot; Calculate probability that the ball with fall in the hole given the angle of the shot and the distance from the hole. &quot;&quot;&quot; rad = angle * np.pi / 180.0 arcsin = np.arcsin((cup_radius - ball_radius)/ distance) return 2 * scipy.stats.norm(0, rad).cdf(arcsin) - 1 . plt.figure(figsize=(16, 6)) ls = np.linspace(0, df.distance.max(), 200) ax = sns.scatterplot( x=&#39;distance&#39;, y=&#39;success_prob&#39;, data=df, s=100, legend=&#39;full&#39; ) for angle in [0.5, 1, 2, 5, 20]: ax.plot( ls, calculate_prob(angle, ls), label=f&quot;Angle={angle}&quot; ) ax.set( xlabel=&#39;Distance from hole(ft)&#39;, ylabel=&#39;Probability of Success&#39; ) ax.legend() . &lt;matplotlib.legend.Legend at 0x7fdfed1ee898&gt; . Let us now add this to our model! . import theano.tensor as tt def calculate_phi(num): &quot;cdf for standard normal&quot; q = tt.erf(num / tt.sqrt(2.0)) # ERF is the Gaussian Error return (1.0 + q) / 2. . with pm.Model() as model: angle_of_shot_radians = pm.HalfNormal(&#39;angle_of_shot_radians&#39;) angle_of_shot_degrees = pm.Deterministic( &#39;angle_of_shot_degrees&#39;, (angle_of_shot_radians * 180.0) / np.pi ) p_ball_goes_in = pm.Deterministic( &#39;p_ball_goes_in&#39;, 2 * calculate_phi( tt.arcsin( (cup_radius - ball_radius)/ df.distance ) / angle_of_shot_radians ) ) - 1 p_success = pm.Binomial( &#39;p_success&#39;, n=df.tries, p=p_ball_goes_in, observed=df.success_count ) . pm.model_to_graphviz(model) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 cluster19 19 angle_of_shot_degrees angle_of_shot_degrees ~ Deterministic angle_of_shot_radians angle_of_shot_radians ~ HalfNormal angle_of_shot_radians&#45;&gt;angle_of_shot_degrees p_ball_goes_in p_ball_goes_in ~ Deterministic angle_of_shot_radians&#45;&gt;p_ball_goes_in p_success p_success ~ Binomial p_ball_goes_in&#45;&gt;p_success with model: trace = pm.sample(4000, tune=1000, chains=4) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... ERROR (theano.gof.opt): Optimization failure due to: local_grad_log_erfc_neg ERROR (theano.gof.opt): node: Elemwise{true_div}(Elemwise{mul,no_inplace}.0, Elemwise{erfc,no_inplace}.0) ERROR (theano.gof.opt): TRACEBACK: ERROR (theano.gof.opt): Traceback (most recent call last): File &#34;/home/goodhamgupta/shubham/blog/_notebooks/.env/lib/python3.6/site-packages/theano/gof/opt.py&#34;, line 2034, in process_node replacements = lopt.transform(node) File &#34;/home/goodhamgupta/shubham/blog/_notebooks/.env/lib/python3.6/site-packages/theano/tensor/opt.py&#34;, line 6789, in local_grad_log_erfc_neg if not exp.owner.inputs[0].owner: AttributeError: &#39;NoneType&#39; object has no attribute &#39;owner&#39; Multiprocess sampling (4 chains in 2 jobs) NUTS: [angle_of_shot_radians] Sampling 4 chains, 0 divergences: 100%|██████████| 20000/20000 [00:10&lt;00:00, 1943.54draws/s] The acceptance probability does not match the target. It is 0.8844154441842546, but should be close to 0.8. Try to increase the number of tuning steps. . pm.summary(trace).head(2) . mean sd hpd_3% hpd_97% mcse_mean mcse_sd ess_mean ess_sd ess_bulk ess_tail r_hat . angle_of_shot_radians 0.027 | 0.000 | 0.026 | 0.027 | 0.0 | 0.0 | 6641.0 | 6641.0 | 6641.0 | 10874.0 | 1.0 | . angle_of_shot_degrees 1.527 | 0.023 | 1.484 | 1.570 | 0.0 | 0.0 | 6641.0 | 6641.0 | 6641.0 | 10874.0 | 1.0 | . pm.plot_posterior(trace[&#39;angle_of_shot_degrees&#39;]) . array([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fdfe4c24f60&gt;], dtype=object) . From the above results, we can see: . PyMC3 has estimated $angle_of_shot_degrees$ to be $1.53 pm 0.023$ | . | The MCSE is almost 0 $ implies$ The simulation has run long enough for the chains to converge. | $r _hat = 1.0$ tells us that the chains have mixed well i.e hairy hedgehog pattern. | . Let&#39;s visualize the fit with this new model: . geo_model_prob = calculate_prob( trace[&#39;angle_of_shot_degrees&#39;].mean(), df.distance ) . sns.set() plt.figure(figsize=(16, 6)) ax = sns.scatterplot(x=&#39;distance&#39;, y=df.success_prob, data=df, s=200, label=&#39;Actual&#39;) sns.scatterplot(x=&#39;distance&#39;, y=df.posterior_success_prob, data=df, label=&#39;Logistic Regression&#39;,ax=ax, color=&#39;red&#39;, s=100) sns.scatterplot(x=&#39;distance&#39;, y=geo_model_prob, data=df, label=&#39;Geometry based &#39;,ax=ax, color=&#39;orange&#39;, s=100) sns.lineplot(x=&#39;distance&#39;, y=df.posterior_success_prob, data=df,ax=ax, color=&#39;red&#39;) sns.lineplot(x=&#39;distance&#39;, y=geo_model_prob, data=df,ax=ax, color=&#39;orange&#39;) ax.set(xlabel=&#39;Distance from hole(ft)&#39;, ylabel=&#39;Probability of Success&#39;) . [Text(0, 0.5, &#39;Probability of Success&#39;), Text(0.5, 0, &#39;Distance from hole(ft)&#39;)] . We can see that the geometry based model fits better than the logistic regression model. | While this model is not completely accurate, it suggests that angle is a good variable to model the problem. Using this model, we can be more confident about extrapolating the data. | For the same 50ft putt, the probability now is: | . import scipy lr_result = np.round( 100 * scipy.special.expit(2.223 + -0.255 * 50).mean(), 5 ) geo_result = np.round( 100 * calculate_prob( trace[&#39;angle_of_shot_degrees&#39;].mean(), 50 ).mean(), 5 ) print( f&quot;Logistic Regression Model: {lr_result}% n&quot; f&quot;Geometry Based Model: {geo_result}%&quot; ) . Logistic Regression Model: 0.00268% Geometry Based Model: 6.40322% . New Data! . Mark Broadie obtained new data about the golfers. Let&#39;s see how our model performs on this new dataset. . First, we&#39;ll look at the summary of the dataset. . # golf putting data from Broadie (2018) new_golf_data = np.array([ [0.28, 45198, 45183], [0.97, 183020, 182899], [1.93, 169503, 168594], [2.92, 113094, 108953], [3.93, 73855, 64740], [4.94, 53659, 41106], [5.94, 42991, 28205], [6.95, 37050, 21334], [7.95, 33275, 16615], [8.95, 30836, 13503], [9.95, 28637, 11060], [10.95, 26239, 9032], [11.95, 24636, 7687], [12.95, 22876, 6432], [14.43, 41267, 9813], [16.43, 35712, 7196], [18.44, 31573, 5290], [20.44, 28280, 4086], [21.95, 13238, 1642], [24.39, 46570, 4767], [28.40, 38422, 2980], [32.39, 31641, 1996], [36.39, 25604, 1327], [40.37, 20366, 834], [44.38, 15977, 559], [48.37, 11770, 311], [52.36, 8708, 231], [57.25, 8878, 204], [63.23, 5492, 103], [69.18, 3087, 35], [75.19, 1742, 24], ]) new_df = pd.DataFrame( new_golf_data, columns=[&#39;distance&#39;, &#39;tries&#39;, &#39;success_count&#39;] ) . new_geo_model_prob = calculate_prob( trace[&#39;angle_of_shot_degrees&#39;].mean(), new_df.distance ) . new_df[&#39;success_prob&#39;] = new_df.success_count / new_df.tries sns.set() plt.figure(figsize=(16, 6)) ax = sns.scatterplot(x=&#39;distance&#39;, y=&#39;success_prob&#39;, data=df, label=&#39;Old Dataset&#39;, s=200) sns.scatterplot(x=&#39;distance&#39;, y=&#39;success_prob&#39;, data=new_df,label=&#39;New Dataset&#39;, s=200, ax=ax) sns.scatterplot(x=&#39;distance&#39;, y=new_geo_model_prob, data=new_df, label=&#39;Geometry based Model &#39;,ax=ax, color=&#39;red&#39;, s=100) ax.set( xlabel=&#39;Distance from hole(ft)&#39;, ylabel=&#39;Probability of Success&#39; ) plt.setp(ax.get_legend().get_texts(), fontsize=&#39;25&#39;) . [None, None, None, None, None, None] . We can see: . Success rate is similar in the 0-20 feet range for both datasets. | Beyond 20 ft, success rate is lower than expected. These attempts are more difficult, even after we have accounted for increased angular precision. | . Moar features! . To get the ball in, along with the angle, we should also need to take into account if the ball was hit hard enough. . From Colin Caroll&#39;s Blog, we have the following: . Mark Broadie made the following assumptions . If a putt goes short or more than 3 feet past the hole, it will not go in. | Golfers aim for 1 foot past the hole | The distance the ball goes, $u$, is distributed as:$$ u sim mathcal{N} left(1 + text{distance}, sigma_{ text{distance}} (1 + text{distance}) right), $$ where we will learn $ sigma_{ text{distance}}$. After working through the geometry and algebra, we get: | . $$P( text{Good shot}) = bigg(2 phi big( frac{sin^{-1}( frac{R-r}{x})}{ sigma_{angle}} big)-1 bigg) bigg( phi bigg( frac{2}{(x+1) sigma_{distance}} bigg) - phi bigg( frac{-1}{(x+1) sigma_{distance}} bigg) bigg)$$ . Let&#39;s write this down in PyMC3 . OVERSHOT = 1.0 DISTANCE_TOLERANCE = 3.0 distances = new_df.distance.values with pm.Model() as model: angle_of_shot_radians = pm.HalfNormal(&#39;angle_of_shot_radians&#39;) angle_of_shot_degrees = pm.Deterministic( &#39;angle_of_shot_degrees&#39;, (angle_of_shot_radians * 180.0) / np.pi ) variance_of_distance = pm.HalfNormal(&#39;variance_of_distance&#39;) p_good_angle = pm.Deterministic( &#39;p_good_angle&#39;, 2 * calculate_phi( tt.arcsin( (cup_radius - ball_radius)/ distances ) / angle_of_shot_radians ) ) - 1 p_good_distance = pm.Deterministic( &#39;p_good_distance&#39;, calculate_phi( (DISTANCE_TOLERANCE - OVERSHOT) / ((distances + OVERSHOT) * variance_of_distance)) - calculate_phi( -OVERSHOT / ((distances + OVERSHOT) * variance_of_distance)) ) p_success = pm.Binomial( &#39;p_success&#39;, n=new_df.tries, p=p_good_angle * p_good_distance, observed=new_df.success_count ) . pm.model_to_graphviz(model) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 cluster31 31 angle_of_shot_degrees angle_of_shot_degrees ~ Deterministic variance_of_distance variance_of_distance ~ HalfNormal p_good_distance p_good_distance ~ Deterministic variance_of_distance&#45;&gt;p_good_distance angle_of_shot_radians angle_of_shot_radians ~ HalfNormal angle_of_shot_radians&#45;&gt;angle_of_shot_degrees p_good_angle p_good_angle ~ Deterministic angle_of_shot_radians&#45;&gt;p_good_angle p_success p_success ~ Binomial p_good_angle&#45;&gt;p_success p_good_distance&#45;&gt;p_success with model: trace = pm.sample(1000, tune=1000, chains=4) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 2 jobs) NUTS: [variance_of_distance, angle_of_shot_radians] Sampling 4 chains, 0 divergences: 100%|██████████| 8000/8000 [00:08&lt;00:00, 969.28draws/s] The number of effective samples is smaller than 25% for some parameters. . pm.summary(trace).head(3) . mean sd hpd_3% hpd_97% mcse_mean mcse_sd ess_mean ess_sd ess_bulk ess_tail r_hat . angle_of_shot_radians 0.013 | 0.000 | 0.013 | 0.013 | 0.0 | 0.0 | 865.0 | 865.0 | 862.0 | 1109.0 | 1.0 | . angle_of_shot_degrees 0.761 | 0.003 | 0.755 | 0.768 | 0.0 | 0.0 | 865.0 | 865.0 | 862.0 | 1109.0 | 1.0 | . variance_of_distance 0.137 | 0.001 | 0.136 | 0.138 | 0.0 | 0.0 | 855.0 | 855.0 | 855.0 | 1186.0 | 1.0 | . pm.plot_posterior(trace[&#39;variance_of_distance&#39;]) . array([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fdff74693c8&gt;], dtype=object) . with model: distance_posterior = pm.sample_posterior_predictive(trace) . 100%|██████████| 4000/4000 [00:04&lt;00:00, 846.25it/s] . def calculate_prob_distance(angle, distance, ls): &quot;&quot;&quot; Calculate the probability the ball will land inside the hole given the variance in angle and distance. NOTE: Adapted from Colin Carroll&#39;s Blog. &quot;&quot;&quot; norm = scipy.stats.norm(0, 1) prob_angle = 2 * norm.cdf( np.arcsin((cup_radius - ball_radius) / ls) / angle) - 1 prob_distance_one = norm.cdf( (DISTANCE_TOLERANCE - OVERSHOT) / ((ls + OVERSHOT) * distance) ) prob_distance_two = norm.cdf(-OVERSHOT / ((ls + OVERSHOT) * distance)) prob_distance = prob_distance_one - prob_distance_two return prob_angle * prob_distance . ls = np.linspace(0, new_df.distance.max(), 200) . new_df[&#39;success_prob&#39;] = new_df.success_count / new_df.tries sns.set() plt.figure(figsize=(16, 6)) ax = sns.scatterplot( x=&#39;distance&#39;, y=&#39;success_prob&#39;, data=new_df, label=&#39;Actual&#39;, s=200 ) sns.scatterplot( x=&#39;distance&#39;, y=new_geo_model_prob, data=new_df, label=&#39;Angle only Model&#39;, ax=ax, color=&#39;red&#39;, s=100 ) sns.scatterplot( x=&#39;distance&#39;, y=calculate_prob_distance( trace[&#39;angle_of_shot_radians&#39;].mean(), trace[&#39;variance_of_distance&#39;].mean(), new_df.distance ), data=new_df, label=&#39;Distance + Angle based Model &#39;, ax=ax, color=&#39;black&#39;, s=100 ) ax.set( xlabel=&#39;Distance from hole(ft)&#39;, ylabel=&#39;Probability of Success&#39; ) plt.setp(ax.get_legend().get_texts(), fontsize=&#39;25&#39;) . [None, None, None, None, None, None] . From the graph, we can conclude that: . The model is good at distance lower than 10 ft and distances higher than 40ft. | There is some mismatch between 10ft to 40ft, but overall this is a good fit. | . What&#39;s the point? . Using Bayesian analysis, we want to be able to quantify the unvertainity with each of our predictions. Since each prediction is a distribution, we can utilize this to see where the putts will fall if they do not fall in the hole. . def simulate_from_distance(trace, distance_to_hole, trials=10_000): n_samples = trace[&#39;angle_of_shot_radians&#39;].shape[0] idxs = np.random.randint(0, n_samples, trials) variance_of_shot = trace[&#39;angle_of_shot_radians&#39;][idxs] variance_of_distance = trace[&#39;variance_of_distance&#39;][idxs] theta = np.random.normal(0, variance_of_shot) distance = np.random.normal(distance_to_hole + OVERSHOT, (distance_to_hole + OVERSHOT) * variance_of_distance) final_position = np.array([distance * np.cos(theta), distance * np.sin(theta)]) made_it = np.abs(theta) &lt; np.arcsin((cup_radius - ball_radius) / distance_to_hole) made_it = made_it * (final_position[0] &gt; distance_to_hole) * (final_position[0] &lt; distance_to_hole + DISTANCE_TOLERANCE) _, ax = plt.subplots() ax.plot(0, 0, &#39;k.&#39;, lw=1, mfc=&#39;black&#39;, ms=150 / distance_to_hole) ax.plot(*final_position[:, ~made_it], &#39;.&#39;, alpha=0.1, mfc=&#39;r&#39;, ms=250 / distance_to_hole, mew=0.5) ax.plot(distance_to_hole, 0, &#39;ko&#39;, lw=1, mfc=&#39;black&#39;, ms=350 / distance_to_hole) ax.set_facecolor(&quot;#e6ffdb&quot;) ax.set_title(f&quot;Final position of {trials:,d} putts from {distance_to_hole}ft. n({100 * made_it.mean():.1f}% made)&quot;) return ax simulate_from_distance(trace, distance_to_hole=10); . Conclusion . We&#39;ve just seen how incorporate subjective knowledge in our models and help them fit cases that are specific to our use-case. . References: . This is heavily inspired by Colin Caroll&#39;s Blog present here | The crux of this post is based on Dr. Gelman&#39;s case study present here. | .",
            "url": "https://goodhamgupta.github.io/blog/jupyter/bayesian/golf_putting/2020/03/12/tutorial.html",
            "relUrl": "/jupyter/bayesian/golf_putting/2020/03/12/tutorial.html",
            "date": " • Mar 12, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "First Post",
            "content": "Yo! . Super excited to finally get my own blog. Hope to write out those long pending articles ASAP now. Stay tuned! .",
            "url": "https://goodhamgupta.github.io/blog/intro/2020/01/14/first-post.html",
            "relUrl": "/intro/2020/01/14/first-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi! I’m Shubham Gupta. Through this blog, I aim to talk a little bit about my current work and potential topics I’m interested in learning. Some of my main projects are available here: . Snowplow Elixir Tracker | Paper Reviews | Doing Bayesian Data Analysis: Notes and Solutions | . You can learn more about my work and contribute on GitHub. .",
          "url": "https://goodhamgupta.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}